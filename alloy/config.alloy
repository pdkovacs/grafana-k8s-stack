// Write your Alloy config here:
loki.write "local" {
  endpoint {
    url = "http://loki:3100/loki/api/v1/push"
  }
}

// discovery.kubernetes allows you to find scrape targets from Kubernetes resources.
// It watches cluster state and ensures targets are continually synced with what is currently running in your cluster.
discovery.kubernetes "pod" {
  role = "pod"
  // Restrict to pods on the node to reduce cpu & memory usage
  selectors {
    role = "pod"
    field = "spec.nodeName=" + coalesce(sys.env("HOSTNAME"), constants.hostname)
  }
}

// discovery.relabel rewrites the label set of the input targets by applying one or more relabeling rules.
// If no rules are defined, then the input targets are exported as-is.
discovery.relabel "pod_logs" {
  targets = discovery.kubernetes.pod.targets

  // Label creation - "namespace" field from "__meta_kubernetes_namespace"
  rule {
    source_labels = ["__meta_kubernetes_namespace"]
    action = "replace"
    target_label = "namespace"
  }

  // Label creation - "pod" field from "__meta_kubernetes_pod_name"
  rule {
    source_labels = ["__meta_kubernetes_pod_name"]
    action = "replace"
    target_label = "pod"
  }

  // Label creation - "container" field from "__meta_kubernetes_pod_container_name"
  rule {
    source_labels = ["__meta_kubernetes_pod_container_name"]
    action = "replace"
    target_label = "container"
  }

  // Label creation -  "app" field from "__meta_kubernetes_pod_label_app_kubernetes_io_name"
  rule {
    source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
    action = "replace"
    target_label = "app"
  }

  // Label creation -  "job" field from "__meta_kubernetes_namespace" and "__meta_kubernetes_pod_container_name"
  // Concatenate values __meta_kubernetes_namespace/__meta_kubernetes_pod_container_name
  rule {
    source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
    action = "replace"
    target_label = "job"
    separator = "/"
    replacement = "$1"
  }

  // Label creation - "__path__" field from "__meta_kubernetes_pod_uid" and "__meta_kubernetes_pod_container_name"
  // Concatenate values __meta_kubernetes_pod_uid/__meta_kubernetes_pod_container_name.log
  rule {
    source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
    action = "replace"
    target_label = "__path__"
    separator = "/"
    replacement = "/var/log/pods/*$1/*.log"
  }

  // Label creation -  "container_runtime" field from "__meta_kubernetes_pod_container_id"
  rule {
    source_labels = ["__meta_kubernetes_pod_container_id"]
    action = "replace"
    target_label = "container_runtime"
    regex = "^(\\S+):\\/\\/.+$"
    replacement = "$1"
  }
}

// loki.source.kubernetes tails logs from Kubernetes containers using the Kubernetes API.
loki.source.kubernetes "pod_logs" {
  targets    = discovery.relabel.pod_logs.output
  forward_to = [loki.process.pod_logs.receiver]
}

// loki.process receives log entries from other Loki components, applies one or more processing stages,
// and forwards the results to the list of receivers in the component's arguments.
loki.process "pod_logs" {
  stage.static_labels {
      values = {
        cluster = "minikube",
      }
  }

  forward_to = [loki.write.local.receiver]
}

prometheus.remote_write "otlp" {
  endpoint {
    url = "http://prometheus-service:80/api/v1/write"
    send_native_histograms = true
  }
}

// Receive traces and metrics with OTEL setup
otelcol.receiver.otlp "default" {
  // configures the default grpc endpoint "0.0.0.0:4317"
  grpc {
    endpoint = "0.0.0.0:4317"
  }

  // configures the default http/protobuf endpoint "0.0.0.0:4318"
  http {
    endpoint = "0.0.0.0:4318"
  }

  output {
    // Receive metrics
    metrics = [otelcol.processor.transform.demote_resource_attrs.input]
    traces  = [
      // Traces to ServiceGraph metrics
      otelcol.connector.servicegraph.default.input,
      // Traces to SpanMetrics
      otelcol.processor.transform.spanmetrics.input,
      // Traces to OTLP endpoint
      otelcol.processor.batch.otlp.input,
    ]
  }
}

otelcol.processor.transform "spanmetrics" {
  error_mode = "ignore"

  trace_statements {
    context = "resource"
    statements = [
      // We keep only the "service.name" and "special.attr" resource attributes,
      // because they are the only ones which otelcol.connector.spanmetrics needs.
      //
      // There is no need to list "span.name", "span.kind", and "status.code"
      // here because they are properties of the span (and not resource attributes):
      // https://github.com/open-telemetry/opentelemetry-proto/blob/v1.0.0/opentelemetry/proto/trace/v1/trace.proto
      `keep_keys(attributes, ["service.name", "special.attr"])`,
    ]
  }
  output {
    traces  = [otelcol.connector.spanmetrics.default.input]
  }
}

otelcol.connector.spanmetrics "default" {
  metrics_flush_interval = "10s"

  histogram {
    // `s` is the unit used by Application Observability
    // https://grafana.com/docs/grafana-cloud/monitor-applications/application-observability/setup/metrics-labels/
    // this creates `traces_span_metrics_duration_seconds` instead of `traces_span_metrics_duration_milliseconds`
    unit = "s"
    // enable `exponential` option to send native histogram metrics
    exponential {
      max_size = 160
    }
    // `explicit` converts to classic histograms
    // explicit { buckets = ["50ms", "100ms", "250ms", "1s", "5s", "10s"]}
  }
  dimension {
    name = "special.attr"
  }

  exemplars {
    enabled = true
  }

  namespace = "traces.spanmetrics"

  output {
    metrics = [otelcol.processor.batch.otlp.input]
  }
}

// Traces to servicegraph metrics
// https://grafana.com/docs/tempo/latest/metrics-from-traces/service_graphs/enable-service-graphs/
otelcol.connector.servicegraph "default" {
  metrics_flush_interval = "10s"
  dimensions = ["http.method", "http.target"]
  // TODO:  Enable when Alloy supports `exponential_histogram_max_size`
  // enable `exponential_histogram_max_size` option to send native histogram metrics
  // exponential_histogram_max_size = 16
  output {
    metrics = [otelcol.processor.batch.otlp.input]
  }
}

otelcol.processor.transform "demote_resource_attrs" {
  error_mode = "ignore"

  metric_statements {
    // error_mode = "propagate"
    context = "datapoint"
    statements = [
      // `set(attributes["debug_resource_name"], resource.attributes["service.name"]) where resource.attributes["service.name"] != nil`,
      `set(attributes["service_name"], resource.attributes["service.name"])`,
      `set(attributes["service_namespace"], resource.attributes["service.namespace"])`,
    ]
  }

  output {
    metrics = [otelcol.processor.batch.otlp.input]
  }
}

otelcol.processor.batch "otlp" {
  output {
    metrics = [otelcol.exporter.prometheus.otlp.input]
    logs = []
    traces = [
      otelcol.exporter.otlp.default.input,
    ]
  }
}

otelcol.exporter.prometheus "otlp" {
  forward_to = [prometheus.remote_write.otlp.receiver]
}

otelcol.exporter.otlp "default" {
  client {
    endpoint = "http://tempo:4317"
    tls {
        insecure             = true
        insecure_skip_verify = true
    }
  }
}
